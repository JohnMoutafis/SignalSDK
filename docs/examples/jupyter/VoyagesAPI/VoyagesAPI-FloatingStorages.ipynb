{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzvBnaNeSoix"
   },
   "source": [
    "# Voyages API Voyages Data Like Use Case\n",
    "\n",
    "## Run this example in [Colab](https://colab.research.google.com/github/SignalOceanSdk/SignalSDK/blob/master/docs/examples/jupyter/VoyagesAPI/VoyagesAPI-FloatingStorages.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\"> \n",
    "     Floating Storages are laden vessels that remain stopped, instead of directly proceeding with the laden part of the voyage and the discharge of the cargo. This is usually performed for trading reasons. The minimum duaration for a stop to be classified as a Floating Storage is 7 days for concluded Stops and 3 days for ongoing ones. \n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: justify\"> \n",
    "    Very often arises the need of conducting an analysis of the total quantity of the quantity of oil that remains in floating storages, either globally or in a specific area/port and for a given time window. This is accommodated by the <b>VoyagesData API</b>.\n",
    "</p>\n",
    "\n",
    "Both `get_voyages_by_advanced_search` and `get_voyages_flat_by_advanced_search` of the Signal SDK facilititate this need. In this example, we will be constructing a dataframe with all the floating storage events of interest, from which a time series of total stored quantitites will be derived.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rk9HODZvSoi5"
   },
   "source": [
    "## Setup\n",
    "Install the Signal Ocean SDK:\n",
    "```\n",
    "pip install signal-ocean\n",
    "```\n",
    "Set your subscription key acquired here: https://apis.signalocean.com/profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install signal-ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x9lAVgQjSoi8"
   },
   "outputs": [],
   "source": [
    "signal_ocean_api_key = '' #replace with your subscription key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JWf1w-CtSoi8"
   },
   "outputs": [],
   "source": [
    "from signal_ocean import Connection\n",
    "from signal_ocean.voyages import VoyagesAPI\n",
    "from signal_ocean.voyages import VesselClass, VesselClassFilter\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FTbI4qFdSoi9"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YltH0cw6Soi-"
   },
   "outputs": [],
   "source": [
    "connection = Connection(signal_ocean_api_key)\n",
    "api = VoyagesAPI(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pB5ASYxSoi-"
   },
   "source": [
    "### Get voyages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8X5V-jF0Soi_"
   },
   "source": [
    "For this tutorial we will retrieve the voyages of VLCC vessels that have ended after January 2020, including those still ongoing. To add other vessel classes like Suezmaxes or Aframaxes, you can retrieve the corresponding VesselClassIDs and add them to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vessel class id for vlcc\n",
    "vc = api.get_vessel_classes(VesselClassFilter('vlcc'))[0]\n",
    "vlcc_id = vc.vessel_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date_from = date(2019, 9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc_list = [vlcc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "voyages_api = api.get_voyages_by_advanced_search(\n",
    "    vessel_class_ids = [*vc_list],\n",
    "    end_date_from = end_date_from\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extract the voyages in a data frame format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyages_df = pd.DataFrame(v.__dict__ for v in voyages_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the voyages into seagoing vessels and storage vessels. We can either include or exclude storage vessels by setting the approproate flag. There are some edge cases wherein the calculated cargo and quantity are null. We treat this case separately by assigning $80\\%$ of their deadweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_storage_vessels = False\n",
    "\n",
    "# extracting floating storage events\n",
    "floating_storage_events_data = []\n",
    "\n",
    "for iVoyage, r in voyages_df.iterrows():\n",
    "    imo = r['imo']\n",
    "    voyage_number = r['voyage_number']\n",
    "    vessel_class = r['vessel_class']\n",
    "    cargo_group = r['cargo_group']\n",
    "    cargo_type_id = r['cargo_type_id']\n",
    "    cargo_type = r['cargo_type']\n",
    "    quantity = r['quantity']\n",
    "    if not quantity:\n",
    "        quantity = r['deadweight'] * 0.8\n",
    "        cargo_group = 'Dirty'\n",
    "        cargo_type_id = 19\n",
    "        cargo_type = 'Crude Oil'\n",
    "\n",
    "    events = r['events']\n",
    "    \n",
    "    for event in events:\n",
    "        if not event.event_details:\n",
    "            continue\n",
    "        if exclude_storage_vessels and e.purpose == 'StorageVessel':\n",
    "            continue\n",
    "        port_name = event.port_name\n",
    "        country = event.country\n",
    "        area_name_level0 = event.area_name_level0\n",
    "        \n",
    "        for event_detail in event.event_details:\n",
    "            if not event_detail.floating_storage_start_date:\n",
    "                continue\n",
    "            floating_storage_start_date = event_detail.floating_storage_start_date\n",
    "            floating_storage_duration = event_detail.floating_storage_duration\n",
    "            \n",
    "            floating_storage_events_data.append([\n",
    "                imo, voyage_number, vessel_class,cargo_group,cargo_type,cargo_type_id,area_name_level0, quantity,\n",
    "                port_name, country, floating_storage_start_date, \n",
    "                floating_storage_duration\n",
    "            ])\n",
    "            \n",
    "floating_storage_events_df = pd.DataFrame(floating_storage_events_data, \n",
    "                                           columns=['imo', 'voyage_number', 'vessel_class','cargo_group', 'cargo_type',\n",
    "                                                    'cargo_type_id','area_name_level0',\n",
    "                                                    'quantity', 'port_name', 'country','floating_storage_start_date', \n",
    "                                                    'floating_storage_duration'\n",
    "                                                   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal uses default thresholds in days after which an event is considered a floating storage event. However, we can also implement custom, stricter thresholds. Here the user can set a higher threshold for a Stop to be considered as a Floating Storage. In this example, we use 20 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the threshold in days to filter floating_storage_duration\n",
    "lower_threshold = 20  \n",
    "floating_storage_events_df = floating_storage_events_df.query(\"floating_storage_duration >= @lower_threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract only the date from the datetime object floating_storage_start_date, since the duration is given in days and, thus, the exact time is not relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_storage_events_df.loc[:, 'floating_storage_start_date'] = floating_storage_events_df.floating_storage_start_date.apply(lambda x: x.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to exclude a specific range of cargoes like fueloils, we add them in an \"excluded\" list. This is mostly revelant for classes smaller than VLCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_cargoes = list(floating_storage_events_df[(floating_storage_events_df.cargo_group == 'Dirty')].cargo_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fueloils = [cargo for cargo in dirty_cargoes if 'fueloil' in cargo.lower().split()]\n",
    "excluded = [*fueloils]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_storage_events_df = floating_storage_events_df[(floating_storage_events_df.cargo_group == 'Dirty') &\n",
    "                                                        (~floating_storage_events_df.cargo_type.isin(excluded))\n",
    "                                                       ].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the new column floating_storage_end_date base on floating_storage_start_date and the duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_storage_events_df['floating_storage_end_date'] = floating_storage_events_df.apply(\n",
    "    lambda r: r['floating_storage_start_date'] + relativedelta(days=r['floating_storage_duration']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform the column names to CamelCase and select the desired columns from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_to_camel(word):\n",
    "    return ''.join(x.capitalize() or '_' for x in word.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_storage_events_df.columns = [*map(snake_to_camel, floating_storage_events_df.columns)]\n",
    "floating_storage_events_df = floating_storage_events_df[['Imo', 'VoyageNumber', 'VesselClass', 'CargoType', 'Quantity',\n",
    "                                                         'AreaNameLevel0',\n",
    "                                                         'PortName', 'Country', 'FloatingStorageStartDate',\n",
    "                                                         'FloatingStorageEndDate'\n",
    "                                                        ]].copy()\n",
    "floating_storage_events_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the date range under consideration for the plots. For the first plot, we focus on the Red Sea area, during 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max dates for consideration\n",
    "date_min = date(2023, 1, 1)\n",
    "date_max = date(2023, 7, 1)\n",
    "\n",
    "delta = (date_max - date_min).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sea_events_df = floating_storage_events_df[floating_storage_events_df.AreaNameLevel0 == 'Red Sea'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the quantity from MT (QMT) to kilo barrels (QKB), using the formula $QKB = \\frac{(QMT/1000) \\times (APIGravity + 131.5)}{141.5\\times 0.159}$. For now we use the API Gravity of Crude Oil, which is 28, but the Cargo API has the exact value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sea_events_df.groupby('VesselClass').Quantity.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_sea_events_df = red_sea_events_df.query(\"VesselClass == 'VLCC'\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_oil_daily_data = []\n",
    "\n",
    "for iDay in range(delta):\n",
    "    curr_date = date_min + relativedelta(days=iDay)\n",
    "    events_on_curr_date = red_sea_events_df[(red_sea_events_df.FloatingStorageStartDate <= curr_date) &\n",
    "                                                   (red_sea_events_df.FloatingStorageEndDate >= curr_date)\n",
    "                                                  ].copy()\n",
    "    if events_on_curr_date.empty:\n",
    "        quantity_on_water = 0\n",
    "    else:\n",
    "        quantity_on_water =  events_on_curr_date.apply(lambda r: \n",
    "                                    r['Quantity'] * (28 + 131.5) / (141.5 * 0.159) / 1_000_000, axis=1).sum()\n",
    "    floating_oil_daily_data.append([curr_date, quantity_on_water])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_oil_series = pd.DataFrame(floating_oil_daily_data, columns=['Date', 'Quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(6, 3))\n",
    "axes1 = fig1.add_axes([0, 0, 1, 1])\n",
    "axes1.plot(floating_oil_series.Date, floating_oil_series.Quantity)\n",
    "\n",
    "axes1.set_title('Floating Storage quantity in Red Sea between January and June, 2023.')\n",
    "axes1.set_xlabel('Date')\n",
    "axes1.set_ylabel('Quantity (ΜBBL)')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second plot, we will show the global monthly trend during 2020, the year the global COVID-19 pandemic broke out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min and max dates for consideration\n",
    "date_min = date(2020, 1, 1)\n",
    "date_max = date(2021, 7, 1)\n",
    "\n",
    "delta = (date_max - date_min).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_oil_daily_data = []\n",
    "\n",
    "for iDay in range(delta):\n",
    "    curr_date = date_min + relativedelta(days=iDay)\n",
    "    events_on_curr_date = floating_storage_events_df[(floating_storage_events_df.FloatingStorageStartDate <= curr_date) &\n",
    "                                                   (floating_storage_events_df.FloatingStorageEndDate >= curr_date)\n",
    "                                                  ].copy()\n",
    "    if events_on_curr_date.empty:\n",
    "        quantity_on_water = 0\n",
    "    else:\n",
    "        quantity_on_water =  events_on_curr_date.apply(lambda r: \n",
    "                                    r['Quantity'] * (28 + 131.5) / (141.5 * 0.159) / 1_000_000, axis=1).sum()\n",
    "    floating_oil_daily_data.append([curr_date, quantity_on_water])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_oil_series = pd.DataFrame(floating_oil_daily_data, columns=['Date', 'Quantity'])\n",
    "floating_oil_series['Date'] = pd.to_datetime(floating_oil_series['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the average quantity of each month to be the month quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_oil_monthly_series = floating_oil_series.resample('M', on='Date').mean().reset_index()\n",
    "floating_oil_monthly_series.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(7, 3))\n",
    "axes2 = fig2.add_axes([0, 0, 1, 1])\n",
    "axes2.plot(floating_oil_monthly_series.Date, floating_oil_monthly_series.Quantity)\n",
    "\n",
    "axes2.set_title('Floating Storage Quantity globally during 2020.')\n",
    "axes2.set_xlabel('Date')\n",
    "axes2.set_ylabel('Quantity (ΜBBL)')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of VoyagesAPI-VoyagesDataLike.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
